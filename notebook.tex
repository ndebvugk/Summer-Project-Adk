
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Comments\_analysis-ZCCcopy}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Free Comments Classifcation}\label{free-comments-classifcation}

This script is designed to carry out a Text analysis of free comments to
classify them into respective categories that they allude to. This will
help draw insights of what elements are of concern to customers by how
much they are highlighted in the customers' comments.

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Background}\label{background}

\subsection{Advisory K}\label{advisory-k}

\textbf{Advisory K} is a wholly owned Zimbabwean firm, and the first and
only official partner in Zimbabwe of \href{www.mercer.com}{Mercer
Global} -- the largest and leading HR Consulting firm in the world.
Through our enriched experience, and our access to global best practice
and solutions in Strategy, Organisation and People solutions, we are the
preeminent firm in the country delivering cohesive strategy and
organisational services focused on the public and parastatal sectors.

\subsubsection{We help our cients know more and do
more}\label{we-help-our-cients-know-more-and-do-more}

Through our five practice areas we help our clients \textbf{Know More
(Research \& Analytics)} so that they can \textbf{Do More (Strategy,
Organisation and People)}. We have a strong belief in the value of data
and in data-driven decision making in an organisation. Our suite of
services is designed to ensure that reliable data drives each part of
any organisation as we take it from Knowing to Doing.\\
\includegraphics{PIcture1.png}

\subsubsection{Research and Analytics}\label{research-and-analytics}

The \textbf{Research and Analytics} Practice Areas provides our clients
with data driven strategic insights. Our data driven approach ensures
\textbf{subjectivism is kept to a minimum} and \textbf{FACTS} are a
major component of our results.

As the \textbf{Research and Analytics} arm of \textbf{Advisory K}, one
of the several products we offer is \textbf{Survey Research}
specialising in: 1. Survey Design and Deployment 2. Survey Administering
3. Survey Analysis and Reporting

\subsection{Survey Analysis}\label{survey-analysis}

Making sense of responses. Tie-up all different responses to come up
with an objective actionable insight of what the respondents perceive as
a group. For all questions in a survey where responses are measured by a
numerical scale this is easy as an index for the perception can be
calculated, for instance a Customer Satisfcation Index (CSI). However,
for free comments, it is difficult to make sense of a large amount of
various responses addressing various concerns.

\subsubsection{Problem Case}\label{problem-case}

How do we identify the concerns and suggestions that customers raise in
the free comment section of the survey?

\subsubsection{Project Objective}\label{project-objective}

Create a model for Text analysis of Customer Satisfaction Survey free
comments and suggestions.

\subsubsection{Project approach}\label{project-approach}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Research on text analysis and the various algorithms used.
\item
  Evaluate the different algorithms and choose one that best addresses
  the need taking into consideration:

  \begin{itemize}
  \tightlist
  \item
    quantity of data
  \item
    scalability
  \item
    continuity
  \end{itemize}
\item
  Implement chosen algorithm from custom template
\item
  Create an executable script that is easy to use
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Set up...}\label{set-up...}

\subsubsection{Tools used.}\label{tools-used.}

I created the model using Jupyter Notebook. This a powerful tool that
combines: * program code * program code output * visualisations *
narrative text * mathematical equations * and media eg pictures.

I chose to do the project in Python since it is the programming language
of choice in Data science. The language also has an extensive range of
ready to use libraries and modules that greatly shorten time to program
and allow you to get results faster.

\textbf{Step 1:} Import the relevant modules and libraries.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{}Importing libraries}
         
         \PY{k+kn}{import} \PY{n+nn}{logging} 
         \PY{k+kn}{import} \PY{n+nn}{gensim} 
         \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd} 
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt} 
         \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns} 
         \PY{k+kn}{import} \PY{n+nn}{nltk}
         
         \PY{k+kn}{from} \PY{n+nn}{gensim} \PY{k}{import} \PY{n}{corpora} 
         \PY{k+kn}{from} \PY{n+nn}{gensim} \PY{k}{import} \PY{n}{models} 
         \PY{k+kn}{from} \PY{n+nn}{gensim} \PY{k}{import} \PY{n}{similarities} 
         \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{corpus} \PY{k}{import} \PY{n}{stopwords} 
         \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{tokenize} \PY{k}{import} \PY{n}{wordpunct\PYZus{}tokenize} 
         \PY{k+kn}{from} \PY{n+nn}{collections} \PY{k}{import} \PY{n}{defaultdict} 
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k}{import} \PY{n}{KMeans} 
         \PY{k+kn}{from} \PY{n+nn}{time} \PY{k}{import} \PY{n}{time} 
         \PY{k+kn}{from} \PY{n+nn}{pprint} \PY{k}{import} \PY{n}{pprint}
         
         \PY{c+c1}{\PYZsh{}Settings}
         
         \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{context}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{notebook}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{style}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{whitegrid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{palette}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{deep}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{rc}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)} 
         \PY{n}{logging}\PY{o}{.}\PY{n}{basicConfig}\PY{p}{(}\PY{n+nb}{format}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}(asctime)s}\PY{l+s+s1}{ : }\PY{l+s+si}{\PYZpc{}(levelname)s}\PY{l+s+s1}{ : }\PY{l+s+si}{\PYZpc{}(message)s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{level}\PY{o}{=}\PY{n}{logging}\PY{o}{.}\PY{n}{INFO}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Data set}\label{data-set}

I will be analysing the comments that were drawn from monthly Customer
Satiscation Surveys from January 2016 to January 2018, for OneClient who
is a healthcare service provider (name changed to not reveal Advisory K
client information). One challenge encountered was getting enough data
since most respondents do not take time to complete the free comments
section of the survey sinces it is optional and respondents are not
obliged to respond. We need to cultivate a culture of giving feedback
without fearing being too critical and judgemental.

\textbf{Step 2:} Loading the data

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} Loading the data from a file}
        \PY{n}{comments} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{all\PYZus{}comments\PYZus{}2016\PYZhy{}2018\PYZhy{}Copy1.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Pre-processing}\label{pre-processing}

\subsubsection{Text processing}\label{text-processing}

To begine our analysis we first have to change the comments into a
\href{http://language.worldofcomputing.net/linguistics/introduction/what-is-corpus.html}{\textbf{Text
Corpus}}. This is a large and structured set of machine readable text
that is representative of or some language, stored and processed
electronically.

To attain our corpus: 1. First remove irrelevant text. This can be done
by utilizing the stopwords function of the nltk module which removes
irrelevant words for a given language. Stopwords can be expanded to
include words or symbols in text that might be considered as irrelevant
by simply updating the function with the words to discard. 2. Secondly
we break up the filtered texts into individual words. This is called
\href{https://www.techopedia.com/definition/13698/tokenization}{Tokenization}.
3. We further filter the list of tokens nested within the list of text
to remove the words with low frequency. These words rarely belong to the
context and do not offer much insight. 4. Create a dictionary that
contains the cleaned data (in form of a list) ready to convert into a
corpus (bag-of-words). Each word in the corpus is represented by a
unique word id and respective weight.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} Text corpus}
        
        \PY{c+c1}{\PYZsh{}put file comments into a list.}
        \PY{n}{document} \PY{o}{=} \PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{comments}\PY{o}{.}\PY{n}{comments}\PY{p}{]}
        \PY{c+c1}{\PYZsh{}pprint(document)}
        
        \PY{c+c1}{\PYZsh{}Removing common words and tokenizing}
        \PY{n}{stop\PYZus{}words} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{n}{stopwords}\PY{o}{.}\PY{n}{words}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{english}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{k}{for} \PY{n}{doc} \PY{o+ow}{in} \PY{n}{document}\PY{p}{:}
            \PY{n}{list\PYZus{}of\PYZus{}words} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{wordpunct\PYZus{}tokenize}\PY{p}{(}\PY{n}{doc}\PY{p}{)}
                            \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{stop\PYZus{}words}\PY{p}{]}
        \PY{n}{stop\PYZus{}words}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{n}{list\PYZus{}of\PYZus{}words}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Removing common words}
        \PY{n}{texts} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{n}{word} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{doc}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}
                 \PY{k}{if} \PY{n}{word} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{stop\PYZus{}words}\PY{p}{]}
                \PY{k}{for} \PY{n}{doc} \PY{o+ow}{in} \PY{n}{document}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Removing words that appear only once}
        \PY{n}{frequency} \PY{o}{=} \PY{n}{defaultdict}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}
        \PY{k}{for} \PY{n}{text} \PY{o+ow}{in} \PY{n}{texts}\PY{p}{:}
            \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{text}\PY{p}{:}
                \PY{n}{frequency}\PY{p}{[}\PY{n}{token}\PY{p}{]} \PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
        \PY{n}{texts} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{n}{token} \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{text}
                 \PY{k}{if} \PY{n}{frequency}\PY{p}{[}\PY{n}{token}\PY{p}{]} \PY{o}{\PYZgt{}} \PY{l+m+mi}{7}\PY{p}{]}
                \PY{k}{for} \PY{n}{text} \PY{o+ow}{in} \PY{n}{texts}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Create dictionary of the document}
        \PY{n}{dictionary} \PY{o}{=} \PY{n}{corpora}\PY{o}{.}\PY{n}{Dictionary}\PY{p}{(}\PY{n}{texts}\PY{p}{)}
        \PY{n}{dictionary}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{allcomments.dict}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Converting document to a vector (bag\PYZhy{}of\PYZhy{}words)}
        \PY{n}{corpus} \PY{o}{=} \PY{p}{[}\PY{n}{dictionary}\PY{o}{.}\PY{n}{doc2bow}\PY{p}{(}\PY{n}{text}\PY{p}{)} \PY{k}{for} \PY{n}{text} \PY{o+ow}{in} \PY{n}{texts}\PY{p}{]}
        \PY{n}{corpora}\PY{o}{.}\PY{n}{MmCorpus}\PY{o}{.}\PY{n}{serialize}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{allcomments.mm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{corpus}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Print done}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Done!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2018-07-26 14:34:26,167 : INFO : adding document \#0 to Dictionary(0 unique tokens: [])
2018-07-26 14:34:26,225 : INFO : built Dictionary(415 unique tokens: ['services', 'polite', 'staff', 'customers', 'space']{\ldots}) from 3251 documents (total 12267 corpus positions)
2018-07-26 14:34:26,227 : INFO : saving Dictionary object under allcomments.dict, separately None
2018-07-26 14:34:26,307 : INFO : saved allcomments.dict
2018-07-26 14:34:26,337 : INFO : storing corpus in Matrix Market format to allcomments.mm
2018-07-26 14:34:26,350 : INFO : saving sparse matrix to allcomments.mm
2018-07-26 14:34:26,352 : INFO : PROGRESS: saving document \#0
2018-07-26 14:34:26,388 : INFO : PROGRESS: saving document \#1000
2018-07-26 14:34:26,433 : INFO : PROGRESS: saving document \#2000
2018-07-26 14:34:26,472 : INFO : PROGRESS: saving document \#3000
2018-07-26 14:34:26,485 : INFO : saved 3251x415 matrix, density=0.881\% (11889/1349165)
2018-07-26 14:34:26,490 : INFO : saving MmCorpus index to allcomments.mm.index

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Done!

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{}pprint(texts)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{}corpus}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} Loading}
        \PY{n}{dictionary} \PY{o}{=} \PY{n}{corpora}\PY{o}{.}\PY{n}{Dictionary}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{allcomments.dict}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2018-07-26 14:34:26,786 : INFO : loading Dictionary object from allcomments.dict
2018-07-26 14:34:26,830 : INFO : loaded allcomments.dict

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Transformation: Term Frequency - Inverse Document Frequency
(TF-IDF)}\label{transformation-term-frequency---inverse-document-frequency-tf-idf}

\textbf{tf-idf} is initialized with a \textbf{bag-of-words} (integer
values) training corpus and takes in a vector, returning another vector
of the same dimensionality. except that features which are rare in the
training corpus will have their value increased. It therefore converts
integer-valued vectors into real-valued ones, while leaving the number
of dimensions intact. It can also, optionally, normalize the resulting
vectors to (Euclidean) unit length.

\paragraph{\texorpdfstring{\emph{Step 1:}}{Step 1:}}\label{step-1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} Initialization}
         \PY{n}{tfidf} \PY{o}{=} \PY{n}{models}\PY{o}{.}\PY{n}{TfidfModel}\PY{p}{(}\PY{n}{corpus}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2018-07-26 14:34:26,865 : INFO : collecting document frequencies
2018-07-26 14:34:26,867 : INFO : PROGRESS: processing document \#0
2018-07-26 14:34:26,874 : INFO : calculating IDF weights for 3251 documents and 414 features (11889 matrix non-zeros)

    \end{Verbatim}

    \paragraph{\texorpdfstring{\emph{Step 2:}}{Step 2:}}\label{step-2}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{} Applying the transformation to the whole corpus}
         \PY{n}{corpus\PYZus{}tfidf} \PY{o}{=} \PY{n}{tfidf}\PY{p}{[}\PY{n}{corpus}\PY{p}{]}
\end{Verbatim}


    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Transformation: Latent Semantic Indexing
(LSI)}\label{transformation-latent-semantic-indexing-lsi}

\href{https://en.wikipedia.org/wiki/Latent_semantic_analysis}{Latent
semantic indexing (LSI)} is an indexing and retrieval method that uses a
mathematical technique called
\href{https://en.wikipedia.org/wiki/Singular-value_decomposition}{singular
value decomposition (SVD)} to identify patterns in the relationships
between terms and concepts contained in an unstructured collection of
text. A key feature of LSI is its ability to extract the conceptual
content of a body of text by establishing associations between those
terms that occur in similar contexts.

\subsubsection{Implemented to correlate semantically related terms that
are latent in a collection of
text.}\label{implemented-to-correlate-semantically-related-terms-that-are-latent-in-a-collection-of-text.}

Transforms documents from either \textbf{bag-of-words} or (preferably)
\textbf{tf-idf-weighted} space into a latent space of a lower
dimensionality.

\textbf{LSI} is unique in that we can continue "training" at any point,
simply by providing more training documents. This is done by incremental
updates to the underlying model, in a process called \emph{online
training}. Because of this feature, the input document stream may even
be infinite - just keep feeding \textbf{LSI} new documents as they
arrive, while using the computed transformation model as read-only in
the meanwhile!

\href{https://radimrehurek.com/gensim/models/lsimodel.html\#module-gensim.models.lsimodel}{\emph{gensim.models.lsimodel}}
contains details for making an \textbf{LSI} model gradually "forget" old
observations in infinite streams, with parameters that can be tweaked to
affect speed, memory footprint and numerical precision of the
\textbf{LSI} algorithm.

\href{https://radimrehurek.com/gensim}{\emph{gensim}} uses a novel
online incremental streamed distributed training algorithm published
\href{https://radimrehurek.com/gensim/tut2.html\#id10}{here}.
\emph{gensim} also executes a stochastic multi-pass algorithm from
\href{https://radimrehurek.com/gensim/tut2.html\#id9}{Halko et al.}
internally, to accelerate in-core part of the computations.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Initializing an LSI transformation}
         \PY{n}{lsi} \PY{o}{=} \PY{n}{models}\PY{o}{.}\PY{n}{LsiModel}\PY{p}{(}\PY{n}{corpus\PYZus{}tfidf}\PY{p}{,} \PY{n}{id2word} \PY{o}{=} \PY{n}{dictionary}\PY{p}{,} \PY{n}{num\PYZus{}topics} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
         \PY{n}{corpus\PYZus{}lsi} \PY{o}{=} \PY{n}{lsi}\PY{p}{[}\PY{n}{corpus\PYZus{}tfidf}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2018-07-26 14:34:26,934 : INFO : using serial LSI version on this node
2018-07-26 14:34:26,935 : INFO : updating model with new documents
2018-07-26 14:34:27,217 : INFO : preparing a new chunk of documents
2018-07-26 14:34:27,255 : INFO : using 100 extra samples and 2 power iterations
2018-07-26 14:34:27,256 : INFO : 1st phase: constructing (415, 110) action matrix
2018-07-26 14:34:27,602 : INFO : orthonormalizing (415, 110) action matrix
2018-07-26 14:34:28,446 : INFO : 2nd phase: running dense svd on (110, 3251) matrix
2018-07-26 14:34:28,715 : INFO : computing the final decomposition
2018-07-26 14:34:28,734 : INFO : keeping 10 factors (discarding 64.643\% of energy spectrum)
2018-07-26 14:34:28,760 : INFO : processed documents up to \#3251
2018-07-26 14:34:28,793 : INFO : topic \#0(14.283): 1.000*"\#name?" + -0.000*"explain" + -0.000*"please" + -0.000*"excellent" + -0.000*"stuff" + -0.000*"go" + 0.000*"find" + 0.000*"got" + 0.000*"rude" + -0.000*"poor"
2018-07-26 14:34:28,795 : INFO : topic \#1(10.397): 0.715*"good." + 0.598*"service" + 0.280*"far" + 0.139*"good" + 0.105*"services" + 0.087*"keep" + 0.057*"slow" + 0.039*"improve" + 0.038*"up." + 0.037*"medication"
2018-07-26 14:34:28,796 : INFO : topic \#2(8.096): 0.359*"medication" + 0.349*"frames" + 0.235*"keep" + 0.230*"time" + 0.229*"waiting" + -0.228*"good." + 0.224*"good" + 0.211*"improve" + 0.205*"need" + -0.202*"far"
2018-07-26 14:34:28,797 : INFO : topic \#3(7.729): -0.518*"keep" + -0.451*"good" + -0.333*"service" + 0.307*"far" + 0.265*"good." + 0.257*"frames" + 0.181*"medication" + 0.124*"waiting" + 0.120*"time" + 0.113*"improve"
2018-07-26 14:34:28,801 : INFO : topic \#4(7.336): -0.734*"medication" + 0.488*"frames" + -0.214*"shortages" + 0.144*"long" + -0.129*"shortage" + -0.116*"pharmacy" + 0.114*"waiting" + 0.113*"time" + 0.111*"keep" + 0.107*"spectacles"

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{lsi}\PY{o}{.}\PY{n}{print\PYZus{}topics}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2018-07-26 14:34:28,812 : INFO : topic \#0(14.283): 1.000*"\#name?" + -0.000*"explain" + -0.000*"please" + -0.000*"excellent" + -0.000*"stuff" + -0.000*"go" + 0.000*"find" + 0.000*"got" + 0.000*"rude" + -0.000*"poor"
2018-07-26 14:34:28,816 : INFO : topic \#1(10.397): 0.715*"good." + 0.598*"service" + 0.280*"far" + 0.139*"good" + 0.105*"services" + 0.087*"keep" + 0.057*"slow" + 0.039*"improve" + 0.038*"up." + 0.037*"medication"
2018-07-26 14:34:28,818 : INFO : topic \#2(8.096): 0.359*"medication" + 0.349*"frames" + 0.235*"keep" + 0.230*"time" + 0.229*"waiting" + -0.228*"good." + 0.224*"good" + 0.211*"improve" + 0.205*"need" + -0.202*"far"
2018-07-26 14:34:28,821 : INFO : topic \#3(7.729): -0.518*"keep" + -0.451*"good" + -0.333*"service" + 0.307*"far" + 0.265*"good." + 0.257*"frames" + 0.181*"medication" + 0.124*"waiting" + 0.120*"time" + 0.113*"improve"
2018-07-26 14:34:28,823 : INFO : topic \#4(7.336): -0.734*"medication" + 0.488*"frames" + -0.214*"shortages" + 0.144*"long" + -0.129*"shortage" + -0.116*"pharmacy" + 0.114*"waiting" + 0.113*"time" + 0.111*"keep" + 0.107*"spectacles"

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:} [(0,
           '1.000*"\#name?" + -0.000*"explain" + -0.000*"please" + -0.000*"excellent" + -0.000*"stuff" + -0.000*"go" + 0.000*"find" + 0.000*"got" + 0.000*"rude" + -0.000*"poor"'),
          (1,
           '0.715*"good." + 0.598*"service" + 0.280*"far" + 0.139*"good" + 0.105*"services" + 0.087*"keep" + 0.057*"slow" + 0.039*"improve" + 0.038*"up." + 0.037*"medication"'),
          (2,
           '0.359*"medication" + 0.349*"frames" + 0.235*"keep" + 0.230*"time" + 0.229*"waiting" + -0.228*"good." + 0.224*"good" + 0.211*"improve" + 0.205*"need" + -0.202*"far"'),
          (3,
           '-0.518*"keep" + -0.451*"good" + -0.333*"service" + 0.307*"far" + 0.265*"good." + 0.257*"frames" + 0.181*"medication" + 0.124*"waiting" + 0.120*"time" + 0.113*"improve"'),
          (4,
           '-0.734*"medication" + 0.488*"frames" + -0.214*"shortages" + 0.144*"long" + -0.129*"shortage" + -0.116*"pharmacy" + 0.114*"waiting" + 0.113*"time" + 0.111*"keep" + 0.107*"spectacles"')]
\end{Verbatim}
            
    \subsubsection{Ouput 1:}\label{ouput-1}

\begin{longtable}[]{@{}l@{}}
\toprule
Top 5 Topics from Latent Semantic Indexing (LSI)\tabularnewline
\midrule
\endhead
\textbf{1}\tabularnewline
\textbf{2}\tabularnewline
\textbf{3}\tabularnewline
\textbf{4}\tabularnewline
\textbf{5}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Saving}
         \PY{n}{lsi}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{allcomments.lsi}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2018-07-26 14:34:28,844 : INFO : saving Projection object under allcomments.lsi.projection, separately None
2018-07-26 14:34:28,848 : INFO : saved allcomments.lsi.projection
2018-07-26 14:34:28,850 : INFO : saving LsiModel object under allcomments.lsi, separately None
2018-07-26 14:34:28,852 : INFO : not storing attribute projection
2018-07-26 14:34:28,854 : INFO : not storing attribute dispatcher
2018-07-26 14:34:28,857 : INFO : saved allcomments.lsi

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} Loading}
         \PY{n}{lsi} \PY{o}{=} \PY{n}{models}\PY{o}{.}\PY{n}{LsiModel}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{allcomments.lsi}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2018-07-26 14:34:28,866 : INFO : loading LsiModel object from allcomments.lsi
2018-07-26 14:34:28,870 : INFO : loading id2word recursively from allcomments.lsi.id2word.* with mmap=None
2018-07-26 14:34:28,871 : INFO : setting ignored attribute projection to None
2018-07-26 14:34:28,876 : INFO : setting ignored attribute dispatcher to None
2018-07-26 14:34:28,878 : INFO : loaded allcomments.lsi
2018-07-26 14:34:28,880 : INFO : loading LsiModel object from allcomments.lsi.projection
2018-07-26 14:34:28,882 : INFO : loaded allcomments.lsi.projection

    \end{Verbatim}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Similarity Queries}\label{similarity-queries}

Used to determine similarity between pairs of documents, or the
similarity between a specific document and a set of other documents
(such as a user query vs. indexed documents).

    \paragraph{\texorpdfstring{\emph{Step1:}}{Step1:}}\label{step1}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} Initializing the query structure: transform corpus to LSI space}
         \PY{c+c1}{\PYZsh{} and index it}
         \PY{n}{index} \PY{o}{=} \PY{n}{similarities}\PY{o}{.}\PY{n}{MatrixSimilarity}\PY{p}{(}\PY{n}{lsi}\PY{p}{[}\PY{n}{corpus}\PY{p}{]}\PY{p}{,} \PY{n}{num\PYZus{}features} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Saving}
         \PY{n}{index}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{allcomments.index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2018-07-26 14:34:28,893 : INFO : creating matrix with 3251 documents and 10 features
/Users/kudakwashe/anaconda3/envs/Virtual1/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
  if np.issubdtype(vec.dtype, np.int):
2018-07-26 14:34:29,280 : INFO : saving MatrixSimilarity object under allcomments.index, separately None
2018-07-26 14:34:29,283 : INFO : saved allcomments.index

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} Loading}
         \PY{n}{index} \PY{o}{=} \PY{n}{similarities}\PY{o}{.}\PY{n}{MatrixSimilarity}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{allcomments.index}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2018-07-26 14:34:29,291 : INFO : loading MatrixSimilarity object from allcomments.index
2018-07-26 14:34:29,296 : INFO : loaded allcomments.index

    \end{Verbatim}

    \paragraph{\texorpdfstring{\emph{Step2:}}{Step2:}}\label{step2}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{c+c1}{\PYZsh{} Performing queries}
         \PY{n}{doc} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pharmacy}\PY{l+s+s2}{\PYZdq{}}
         \PY{n}{vec\PYZus{}bow} \PY{o}{=} \PY{n}{dictionary}\PY{o}{.}\PY{n}{doc2bow}\PY{p}{(}\PY{n}{doc}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Convert the query to LSI spaace}
         \PY{n}{vec\PYZus{}lsi} \PY{o}{=} \PY{n}{lsi}\PY{p}{[}\PY{n}{vec\PYZus{}bow}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} Perform a similarity query against the corpus}
         \PY{n}{sims} \PY{o}{=} \PY{n}{index}\PY{p}{[}\PY{n}{vec\PYZus{}lsi}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} Ranking the comments by their weights of similarity}
         \PY{n}{sims} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{sims}\PY{p}{)}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{item}\PY{p}{:} \PY{o}{\PYZhy{}}\PY{n}{item}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Printing the associated comments:}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comments Rank \PYZsh{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{:}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{Weights }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Raw Text: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,}
                     \PY{n}{sims}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{document}\PY{p}{[}\PY{n}{sims}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Comments Rank \#1:	Weights 1.0
Raw Text: pharmacy in town doesn't  have medicine.

Comments Rank \#2:	Weights 0.999454915523529
Raw Text: no pharmacy in chipinge

Comments Rank \#3:	Weights 0.999454915523529
Raw Text: establish a pharmacy in chipinge

Comments Rank \#4:	Weights 0.9993054270744324
Raw Text: pharmacy should have medication.

Comments Rank \#5:	Weights 0.9977983832359314
Raw Text: they should open a pharmacy in chipinge 


    \end{Verbatim}

    \subsubsection{Output 2:}\label{output-2}

\begin{longtable}[]{@{}l@{}}
\toprule
\textbf{1}\tabularnewline
\textbf{2}\tabularnewline
\textbf{3}\tabularnewline
\textbf{4}\tabularnewline
\textbf{5}\tabularnewline
\bottomrule
\end{longtable}

    \subsubsection{Observations and Results}\label{observations-and-results}

At this point, we observe that \textbf{LSI} provides topic models with a
coherent semantic reference. In this instance it is the best model to
use compared to another model known as \textbf{Latent Dirichlet
Allocation (LDA)} because \textbf{LSI} is less sensitive to the amount
of data than \textbf{LDA}, given we are working with just over 3000
comments to conduct our analysis. However, with more comments
(apprximately 10000 comments) \textbf{LDA} tends to provide more better
results than \textbf{LSI}

\textbf{LSI} with the ability to query for similarity allows to bring
out insights on what the comments highlight about a certain topics.

Given enough computing power and other relevant resources,
\textbf{Clustering} using the K-means can be done to better categorize
the comments with respect to specified categories. These can then be
also categorized by sentiment (positive, negative, neutral).

    \subsubsection{Value Proposition}\label{value-proposition}

\begin{itemize}
\tightlist
\item
  Increase efficiency of analysing surveys
\item
  Cost reductions to the firm in terms of billable hours to analyse
  comments
\item
  Reduce subjectivity of an individual's interpretation of comments
\end{itemize}

    \subsubsection{Challenges and
Opportunities}\label{challenges-and-opportunities}

The project was challenging as I was not aware of the concept of text or
sentiment analysis. However, it proposed a valuable opportunity to learn
something new and required me to do some programming, which for a MIS
major I had not managed to do all summer. As an aspiring data analyst
and business consultant, I got to learn more about the different
branches of analytics and get a picture of the opportunities that exist
there.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
